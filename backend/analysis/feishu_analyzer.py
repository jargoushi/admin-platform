"""
é£ä¹¦é“¾æ¥åˆ†æå™¨ - ç®€åŒ–ç‰ˆ
åªè·å–URLå’Œæ ‡é¢˜
"""
import asyncio
import json
import re
from collections import Counter
from pathlib import Path
from typing import List, Tuple

import sys
sys.path.insert(0, str(Path(__file__).parent.parent))

from app.util.Playwright_util import PlaywrightUtil


def read_urls_from_file(file_path: str) -> List[str]:
    """ä»æ–‡ä»¶è¯»å–URLåˆ—è¡¨"""
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()

    pattern = r'https://[a-zA-Z0-9]+\.feishu\.cn/[^\s]+'
    matches = re.findall(pattern, content)
    return [url.strip() for url in matches if url.strip()]


def normalize_url(url: str) -> str:
    """æ ‡å‡†åŒ–URLï¼Œå»é™¤æŸ¥è¯¢å‚æ•°"""
    return url.split('?')[0] if '?' in url else url


def check_duplicates(urls: List[str]) -> Tuple[List[str], dict]:
    """æ£€æŸ¥é‡å¤çš„URL"""
    normalized_counter = Counter([normalize_url(url) for url in urls])
    duplicates = {url: count for url, count in normalized_counter.items() if count > 1}
    unique_urls = list(set([normalize_url(url) for url in urls]))
    return unique_urls, duplicates


async def fetch_title(url: str, util: PlaywrightUtil) -> dict:
    """è·å–URLçš„æ ‡é¢˜"""
    result = {'url': url, 'title': None}

    try:
        page = util._get_page()
        await page.goto(url, timeout=10000, wait_until='domcontentloaded')
        await asyncio.sleep(0.5)

        title = await page.title()
        # ç§»é™¤ " - é£ä¹¦äº‘æ–‡æ¡£" åç¼€
        if title and ' - é£ä¹¦äº‘æ–‡æ¡£' in title:
            title = title.replace(' - é£ä¹¦äº‘æ–‡æ¡£', '')
        result['title'] = title

    except Exception as e:
        result['title'] = f"è·å–å¤±è´¥: {str(e)}"

    return result


async def analyze_urls(file_path: str, test_count: int = None):
    """åˆ†æURLæ–‡ä»¶"""
    print("=" * 60)
    print("é£ä¹¦é“¾æ¥åˆ†æ")
    print("=" * 60)

    urls = read_urls_from_file(file_path)
    print(f"\nğŸ“Š æ€»URL: {len(urls)}")

    unique_urls, duplicates = check_duplicates(urls)
    print(f"ğŸ“Š å»é‡å: {len(unique_urls)}")

    if duplicates:
        print(f"\nâš ï¸ é‡å¤URL: {len(duplicates)} ä¸ª")
    else:
        print("\nâœ… æ— é‡å¤URL")

    test_urls = unique_urls if test_count is None else unique_urls[:test_count]
    total = len(test_urls)
    print(f"\nğŸ” è·å–æ ‡é¢˜ä¸­ (å…± {total} ä¸ª)...")

    util = PlaywrightUtil(headless=True)
    all_results = []

    try:
        await util.start_browser()

        for i, url in enumerate(test_urls, 1):
            result = await fetch_title(url, util)
            all_results.append(result)

            if i % 10 == 0 or i == total:
                print(f"[{i}/{total}] {i*100//total}%")

            await asyncio.sleep(0.3)

    finally:
        await util.close_browser()

    print("\nâœ… å®Œæˆ")
    return {'unique_url_list': unique_urls, 'results': all_results}


if __name__ == '__main__':
    data_file = Path(__file__).parent.parent / 'data' / 'ç”Ÿè´¢1226.txt'

    if not data_file.exists():
        print(f"æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
    else:
        result = asyncio.run(analyze_urls(str(data_file), test_count=None))

        # ä¿å­˜ç»“æœ
        results_file = Path(__file__).parent / 'feishu_results.json'
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(result['results'], f, ensure_ascii=False, indent=2)
        print(f"ç»“æœå·²ä¿å­˜è‡³: {results_file}")
